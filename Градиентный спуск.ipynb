{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aacy0NIewvsFfa91n251ar5RzmCz3vDe","timestamp":1699050816042}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 3.2 Градиентный спуск\n","\n","В этом задании нам предстоит реализовать классический алгоритм градиентного спуска для обучения модели логистической регрессии.\n","\n","Алгоритм выполнения этого задания следующий:\n","\n","* На основе посчитанных в первом задании частных производных, напишем функцию подсчета градиента бинарной кросс-энтропии по параметрам модели\n","\n","* Напишем функцию обновления весов по посчитанным градиентам\n","\n","* Напишем функцию тренировки модели\n","\n","Замечание:\n","Тренировка модели проводится в несколько циклов, в рамках каждого из которых мы обновим веса модели, основываясь на предсказании для **каждого** объекта из датасета. Такие циклы называются *эпохами*. То есть одна эпоха - это набор обновлений весов, реализованный согласно посчитанным для каждого объекта из датасета ошибкам модели.\n","\n","Вам необходимо реализовать обучение модели в несколько эпох. Их количество задается параметром функции. В рамках каждой эпохи необходимо пройти циклом по всем объектам обучающей выборки и обновить веса модели."],"metadata":{"id":"zVKa9zcWdm-p"}},{"cell_type":"markdown","source":["Шаблон кода для заполнения:"],"metadata":{"id":"zrTqdyBid_G8"}},{"cell_type":"code","source":["import numpy as np\n","\n","np.random.seed(42)\n","def sigmoid(x):\n","    return 1/(1 + np.exp(-x))\n","def gradient(y_true: int, y_pred: float, x: np.array) -> np.array:\n","    grdnt = []\n","    for i in range(len(x)):\n","      expr = x[i]*((1 - y_true) * y_pred - y_true * (1.0 - y_pred))\n","      grdnt.append(expr)\n","    grdnt.append((1 - y_true) * y_pred - y_true * (1.0 - y_pred))\n","    grad = np.array(grdnt)\n","    return grad\n","\n","def update(alpha: np.array, gradient: np.array, lr: float):\n","    alpha_new = np.subtract(alpha, gradient * lr)\n","    return alpha_new\n","\n","def train(\n","    alpha0: np.array, x_train: np.array, y_train: np.array, lr: float, num_epoch: int\n","):\n","    alpha = alpha0.copy()\n","    for epo in range(num_epoch):\n","        for i, x in enumerate(x_train):\n","            sump = alpha[len(alpha) - 1]\n","            for j in range (len(x)):\n","                sump += x[j] * alpha[j]\n","            y_pred = sigmoid(sump)\n","            grad = gradient(y_train[i], y_pred, x)\n","            alpha = update(alpha, grad, lr)\n","    return alpha"],"metadata":{"id":"CCM4EIh_d8-n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Замечания:\n","\n","1. В случае, если у Вас возникли сложности с выполнением первого задания и, как следствие, у Вас не выходит сделать это, мы рекомендуем подробно ознакомиться с главой **Производные $\\frac{\\partial H}{\\partial \\omega_i}$** нашей [лекции](https://colab.research.google.com/drive/1xjX_YnXcRr8HSiYLByMHxEIAADqs7QES?usp=sharing).\n","\n","2. Обращайте внимание на названия и порядок аргументов в сдаваемых на проверку функциях - они должны совпадать с тем, что указано в шаблоне кода.\n","\n","3. Обратите внимание, что матрица объект-признак в описании параметров функций обозначает переменную типа numpy.array(), каждый элемент которой - объект типа numpy.array() - вектор признаков соответствующего объекта.\n","\n","4. Считайте, что свободный коэффициент a0 находится **в конце** списка alpha."],"metadata":{"id":"zDcPxeLueFIk"}}]}